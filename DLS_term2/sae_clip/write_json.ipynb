{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f213f5-fe08-430a-bab7-a6935ca2e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d758eb-6fd0-41f3-88ce-9262b5efc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Vanilla_(ReLU)_v1\": {\n",
    "        \"clip_model\": \"openai/clip-vit-base-patch16\",\n",
    "        \"hook_layer\": 10,\n",
    "        \"hook_module\": \"self_attn\",\n",
    "        \"expansion_factor\": 64,\n",
    "        \"k\": None,\n",
    "        \"geom_dec_bias\": False,\n",
    "        \"data_path\": \"data/vanilla\",\n",
    "        \"chpt_path\": \"clip-vit-base-patch16_sae.pt\",\n",
    "    },\n",
    "    \"TopK_v1\": {\n",
    "        \"clip_model\": \"openai/clip-vit-base-patch16\",\n",
    "        \"hook_layer\": 10,\n",
    "        \"hook_module\": \"self_attn\",\n",
    "        \"expansion_factor\": 64,\n",
    "        \"k\": 32,\n",
    "        \"geom_dec_bias\": False,\n",
    "        \"data_path\": \"data/top32\",\n",
    "        \"chpt_path\": \"clip-vit-base-patch16_sae-top32.pt\",\n",
    "    },\n",
    "    \"Vanilla_(ReLU)_v2\": {\n",
    "        \"clip_model\": \"openai/clip-vit-base-patch16\",\n",
    "        \"hook_layer\": 10,\n",
    "        \"hook_module\": \"mlp\",\n",
    "        \"expansion_factor\": 64,\n",
    "        \"k\": None,\n",
    "        \"geom_dec_bias\": False,\n",
    "        \"data_path\": \"data/vanilla-v2\",\n",
    "        \"chpt_path\": \"clip-vit-base-patch16_sae-v2.pt\",\n",
    "    },\n",
    "    \"TopK_v2\": {\n",
    "        \"clip_model\": \"openai/clip-vit-base-patch16\",\n",
    "        \"hook_layer\": 10,\n",
    "        \"hook_module\": \"mlp\",\n",
    "        \"expansion_factor\": 64,\n",
    "        \"k\": 32,\n",
    "        \"geom_dec_bias\": False,\n",
    "        \"data_path\": \"data/top32-v2\",\n",
    "        \"chpt_path\": \"clip-vit-base-patch16_sae-top32-v2.pt\",\n",
    "    },\n",
    "    \"Vanilla_(ReLU)_v3\": {\n",
    "        \"clip_model\": \"openai/clip-vit-base-patch16\",\n",
    "        \"hook_layer\": 10,\n",
    "        \"hook_module\": \"mlp\",\n",
    "        \"expansion_factor\": 64,\n",
    "        \"k\": None,\n",
    "        \"geom_dec_bias\": True,\n",
    "        \"data_path\": \"data/vanilla-v3\",\n",
    "        \"chpt_path\": \"clip-vit-base-patch16_sae-v3.pt\",\n",
    "    },\n",
    "}\n",
    "with open(\"models.json\", \"w\") as ouf:\n",
    "    ouf.write(json.dumps(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857a5906-6616-4847-95cc-881ee9e5e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aecd4d9e-5ae2-4bda-8765-fb8abf72cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_model(**kwargs):\n",
    "    chpt_path = kwargs.pop(\"chpt_path\")\n",
    "    model = SAEonCLIP(**kwargs)\n",
    "    model.sae.load_state_dict(torch.load(chpt_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbb1890-efa9-43a9-8781-679b9be17c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models.json\", \"r\") as inf:\n",
    "    models = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7ac617-20e5-41d2-8085-d9485281b996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clip_model': 'openai/clip-vit-base-patch16',\n",
       " 'hook_layer': 10,\n",
       " 'hook_module': 'mlp',\n",
       " 'expansion_factor': 64,\n",
       " 'k': None,\n",
       " 'geom_dec_bias': True,\n",
       " 'data_path': 'data/vanilla-v3',\n",
       " 'chpt_path': 'clip-vit-base-patch16_sae-v3.pt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = models[\"Vanilla_(ReLU)_v3\"]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc030ef-f07e-431a-ae47-23843b7ca641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAEonCLIP(\n",
       "  (clip): CLIPModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (position_embedding): Embedding(77, 512)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (position_embedding): Embedding(197, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (sae): SparseAutoEncoder(\n",
       "    (activation_fun): ReLU()\n",
       "    (encoder): Linear(in_features=768, out_features=49152, bias=True)\n",
       "    (decoder): Linear(in_features=49152, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del params[\"data_path\"]\n",
    "model = fetch_model(**params, device=\"cpu\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b262826-438c-4a75-bf98-86301350e6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
